{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import xml.etree.cElementTree as et\n",
    "from lxml import etree\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()  # just the language with no model\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All news articles\n",
    "df=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/parsed_xml.pkl')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Newspaper'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify \"regulatory sections\" (a sentence with \"regulat*\" and its neighbor sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove multiple spaces\n",
    "def remove_spaces(text):\n",
    "    text=re.sub(' +',' ',text).strip()\n",
    "    text=text.replace('\\n',' ').replace('\\r',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify the sentence with \"*regulat*\" and a sentence before and after (expanded regulatory sentences)\n",
    "def extractSentenceBeforeAfter(text):\n",
    "    sentSet=set()\n",
    "    text=remove_spaces(text)\n",
    "    doc=nlp(text)\n",
    "    sentList=list(doc.sents)\n",
    "    for i in range(0, len(sentList)):\n",
    "        sent=sentList[i].text.strip()\n",
    "        if len(re.findall('regulat',sent.lower()))>0:\n",
    "            sentSet.add(sent)\n",
    "            if i>0:\n",
    "                sentSet.add(sentList[i-1].text.strip())\n",
    "            if i<len(sentList)-1:\n",
    "                sentSet.add(sentList[i+1].text.strip())\n",
    "    sentText=' '.join(sentSet)\n",
    "    return sentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822737\n"
     ]
    }
   ],
   "source": [
    "# Extract expanded regulatory sentences\n",
    "regsents_expand=[]\n",
    "for text in df['Text']:\n",
    "    new=extractSentenceBeforeAfter(text)\n",
    "    regsents_expand.append(new)\n",
    "print(len(regsents_expand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Abe's critics say he failed to address structural problems such as a declining population and onerous government regulation in order to boost growth potential. Without those changes -- which Mr. Abe termed the \"third arrow\" of Abenomics -- BOJ policy wasn't enough, they say. Japan has been dealing with all of those problems since the 1990s -- and the BOJ already set the same goal of overshooting the 2% target four years ago, with little effect.\n"
     ]
    }
   ],
   "source": [
    "print(regsents_expand[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RegSentsExpand']=regsents_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID  RegSentExpandLen\n",
      "785285  1824047135             38644\n",
      "785284  1824047069             38518\n",
      "780573  1798874942             28365\n",
      "790351  1859383206             27524\n",
      "788561  1845766676             25873\n",
      "793733  1886296496             25038\n",
      "791594  1867515064             24706\n",
      "790140  1857676896             24410\n",
      "698039   847586480             24403\n",
      "792784  1877776183             24136\n"
     ]
    }
   ],
   "source": [
    "df['RegSentExpandLen']=df['RegSentsExpand'].str.len()\n",
    "print(df.sort_values('RegSentExpandLen',ascending=False)[['ID','RegSentExpandLen']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                                            Title  Type  \\\n",
      "0  294326637             1984: IT WAS THE YEAR OF THE BIG LIE  News   \n",
      "1  294308147  CONTROLS LIFTED ON ABOUT HALF OF US NATURAL GAS  News   \n",
      "2  294323196              COURT UPHOLDS DIABLO CANYON LICENSE  News   \n",
      "3  294311708                      HUNT'S IMMACULATE RECEPTION  News   \n",
      "4  294262284                          LEGISLATIVE REPORT CARD  News   \n",
      "\n",
      "   StartDate     EndDate                                               Text  \\\n",
      "0 1985-01-01  1985-01-01  ART BUCHWALD Art Buchwald is a syndicated colu...   \n",
      "1 1985-01-01  1985-01-01  After 30 years of strict federal control, pric...   \n",
      "2 1985-01-01  1985-01-01  A federal appeals court yesterday upheld the N...   \n",
      "3 1985-01-01  1985-01-01  COTTON BOWL '85 / JOHN ROBINSON John Robinson ...   \n",
      "4 1985-01-01  1985-01-01  Much of Beacon Hill's 1984 legislative activit...   \n",
      "\n",
      "  TextWordCount                          PubTitle  SourceType  Year  Month  \\\n",
      "0           422  Boston Globe (pre-1997 Fulltext)  Newspapers  1985      1   \n",
      "1           751  Boston Globe (pre-1997 Fulltext)  Newspapers  1985      1   \n",
      "2           238  Boston Globe (pre-1997 Fulltext)  Newspapers  1985      1   \n",
      "3          1142  Boston Globe (pre-1997 Fulltext)  Newspapers  1985      1   \n",
      "4          1570  Boston Globe (pre-1997 Fulltext)  Newspapers  1985      1   \n",
      "\n",
      "      Newspaper                                     RegSentsExpand  \\\n",
      "0  Boston Globe  \"Deregulation of natural gas will lower your h...   \n",
      "1  Boston Globe  Because rates can be adjusted only after a pri...   \n",
      "2  Boston Globe  A federal appeals court yesterday upheld the N...   \n",
      "3  Boston Globe  A man less secure in his prospects might have ...   \n",
      "4  Boston Globe  Legislation which would reduce employers' unem...   \n",
      "\n",
      "   RegSentExpandLen  \n",
      "0               193  \n",
      "1               596  \n",
      "2               419  \n",
      "3               577  \n",
      "4              1299  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','RegSentsExpand']].to_pickle('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/allRegSentsExpand.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Alternative approach 1: identify sentences with \"regulat*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify sentences with \"*regulat*\"\n",
    "def extractSentence(text):\n",
    "    sentSet=set()    # use set to avoid duplicated sentences\n",
    "    text=remove_spaces(text)\n",
    "    doc=nlp(text)\n",
    "    for item in doc.sents:\n",
    "        sent=item.text.strip()\n",
    "        if len(re.findall('regulat',sent.lower()))>0:\n",
    "            sentSet.add(sent)\n",
    "    sentText=' '.join(sentSet)\n",
    "    return sentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract regulatory sentences\n",
    "regsents=[]\n",
    "for text in df['Text']:\n",
    "    new=extractSentence(text)\n",
    "    regsents.append(new)\n",
    "print(len(regsents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RegSents']=regsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['RegSents'][200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify titles with \"*regulat*\"\n",
    "def extractTitle(title):\n",
    "    title=remove_spaces(str(title))\n",
    "    if len(re.findall('regulat',title.lower()))>0:\n",
    "        regTitle=title\n",
    "    else:\n",
    "        regTitle=\"\"\n",
    "    return regTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract regulatory titles\n",
    "regtitles=[]\n",
    "for title in df['Title']:\n",
    "    new=extractTitle(title)\n",
    "    regtitles.append(new)\n",
    "\n",
    "print(len(regtitles), regtitles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RegTitles']=regtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['RegTitles']!='']['ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['RegTitles']!=''][['ID','RegTitles']][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','RegSents','RegTitles']].to_pickle('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/allRegSents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/allRegNews_RegSents.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Alternative approach 2: identify sections with \"regulat*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print one XML example\n",
    "def print_xml(ID):\n",
    "    tree = etree.parse(filePath+ID+'.xml')\n",
    "    xml = etree.tostring(tree, encoding=\"unicode\", pretty_print=True)\n",
    "    print(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath='/home/ec2-user/SageMaker/data/corpus/regnews/'\n",
    "filePath2='/home/ec2-user/SageMaker/data/corpus/regnews2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See an example\n",
    "print_xml('398770257')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove html tags from a string\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract sections with \"regulat\"\n",
    "def extractSection(text):\n",
    "    secSet=set()\n",
    "    secStartPos=[m.end() for m in re.finditer('&lt;p&gt',text)]\n",
    "    secStartPos=secStartPos+[m.end() for m in re.finditer('<p>',text)]\n",
    "    secStartPos=secStartPos+[m.end() for m in re.finditer('<ul>',text)]\n",
    "    secEndPos=[m.start() for m in re.finditer('&lt;/p&gt',text)]\n",
    "    secEndPos=secEndPos+[m.start() for m in re.finditer('</p>',text)]\n",
    "    secEndPos=secEndPos+[m.start() for m in re.finditer('</ul>',text)]\n",
    "    regPos=[m.start() for m in re.finditer('regulat',text.lower())]\n",
    "\n",
    "    if len(regPos)>0:\n",
    "        for reg in regPos:\n",
    "            start=-99999\n",
    "            end=99999\n",
    "            for sec in secStartPos:\n",
    "                if (sec-reg<=0) & (sec>start):\n",
    "                    start=sec\n",
    "            for sec in secEndPos:\n",
    "                    if (sec-reg>=0) & (sec<end):\n",
    "                        end=sec\n",
    "            secSet.add(text[start:end])\n",
    "        secText=\" \".join(secSet)\n",
    "    else:\n",
    "        secText=''\n",
    "    return secText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sections\n",
    "sections=[] \n",
    "for ID in df['ID']:\n",
    "    xmlp = et.XMLParser(encoding=\"UTF-8\")\n",
    "    try:\n",
    "        file=filePath+ID+'.xml'\n",
    "        parsed_xml = et.parse(file,parser=xmlp)\n",
    "    except:\n",
    "        file=filePath2+ID+'.xml'\n",
    "        parsed_xml = et.parse(file,parser=xmlp)\n",
    "    root = parsed_xml.getroot()\n",
    "    for child in root.findall('Obj'):\n",
    "        if root.find('TextInfo')!=None:\n",
    "            for node in root.iter('Text'):\n",
    "                text=node.text\n",
    "                section=extractSection(text) \n",
    "                section=remove_html_tags(section)\n",
    "                sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sections))\n",
    "print(sections[50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RegSections']=sections\n",
    "df['RegSectionLen']=df['RegSections'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sort_values('RegSectionLen',ascending=False)[df['RegSectionLen']>0][['ID','RegSectionLen']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','RegSections','RegSectionLen']].to_pickle('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/allRegSections.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py",
   "language": "python",
   "name": "my_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
