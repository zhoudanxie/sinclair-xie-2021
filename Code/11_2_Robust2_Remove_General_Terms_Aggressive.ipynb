{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Check: Remove General Terms from Top 100 Noun Chunks in Each Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/nltk_data',\n",
       " '/home/ec2-user/SageMaker/.conda/envs/my_py/nltk_data',\n",
       " '/home/ec2-user/SageMaker/.conda/envs/my_py/share/nltk_data',\n",
       " '/home/ec2-user/SageMaker/.conda/envs/my_py/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Regulatory Sections and Noun Chunks with Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10645 entries, 0 to 10644\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   nc_code      10645 non-null  int64 \n",
      " 1   noun_chunks  10645 non-null  object\n",
      " 2   rin          10645 non-null  object\n",
      " 3   area         10185 non-null  object\n",
      " 4   area_no      10645 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 415.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Noun chunks with areas\n",
    "nounchunks_area=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/DictionaryOfRegulatoryNounChunks.csv')\n",
    "print(nounchunks_area.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nc_code</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>rin</th>\n",
       "      <th>area</th>\n",
       "      <th>area_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>180-day exclusivity</td>\n",
       "      <td>0910-AC11</td>\n",
       "      <td>{1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1983 amendment</td>\n",
       "      <td>2115-AB72</td>\n",
       "      <td>{2}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1988 amendment</td>\n",
       "      <td>1205-AB05</td>\n",
       "      <td>{4}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1990 farm bill</td>\n",
       "      <td>0584-AB28</td>\n",
       "      <td>{1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1993 provision</td>\n",
       "      <td>0970-AB32,3206-AG31</td>\n",
       "      <td>{1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nc_code          noun_chunks                  rin area  area_no\n",
       "0        0  180-day exclusivity            0910-AC11  {1}        1\n",
       "1        1       1983 amendment            2115-AB72  {2}        1\n",
       "2        2       1988 amendment            1205-AB05  {4}        1\n",
       "3        3       1990 farm bill            0584-AB28  {1}        1\n",
       "4        4       1993 provision  0970-AB32,3206-AG31  {1}        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nounchunks_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1385 entries, 0 to 1384\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   noun_chunks      1385 non-null   object \n",
      " 1   area_no          1385 non-null   int64  \n",
      " 2   area_name        1385 non-null   object \n",
      " 3   remove_dda_less  258 non-null    float64\n",
      " 4   remove_dda_more  449 non-null    float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 54.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Remove area-specific general terms\n",
    "nounchunks_remove=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/Top 100 Area-specific Noun Chunks Removed.csv')\n",
    "print(nounchunks_remove.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    449\n",
      "Name: remove_dda, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Specify which removal appraoch to use\n",
    "nounchunks_remove=nounchunks_remove.rename(columns={'remove_dda_more':'remove_dda'})\n",
    "print(nounchunks_remove['remove_dda'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10645 entries, 0 to 10644\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   nc_code      10645 non-null  int64  \n",
      " 1   noun_chunks  10645 non-null  object \n",
      " 2   rin          10645 non-null  object \n",
      " 3   area         10185 non-null  object \n",
      " 4   area_no      10645 non-null  int64  \n",
      " 5   remove_dda   449 non-null    float64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 582.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "nounchunks_area=nounchunks_area.merge(nounchunks_remove[['noun_chunks','remove_dda']],on='noun_chunks',how='left')\n",
    "print(nounchunks_area.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9736\n"
     ]
    }
   ],
   "source": [
    "# Convert to dictionary\n",
    "nounchunks_area=nounchunks_area[(nounchunks_area['area_no']>0) & (nounchunks_area['remove_dda']!=1)].set_index('noun_chunks')\n",
    "nounchunks_area_dict=nounchunks_area.to_dict()['area']\n",
    "print(len(nounchunks_area_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 822737 entries, 0 to 822736\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   ID                           822737 non-null  object\n",
      " 1   RegSentsExpand               822737 non-null  object\n",
      " 2   NounChunksMatch              822737 non-null  int64 \n",
      " 3   NounChunkMatchWords          822737 non-null  object\n",
      " 4   NounChunkMatchWordsFiltered  822737 non-null  object\n",
      " 5   NounChunkMatchFiltered       822737 non-null  int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 37.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Expanded reg sentences with matched noun chunks\n",
    "df_regSentsExpand=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/RegSentsExpand_NounChunks3.pkl')\n",
    "print(df_regSentsExpand.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 788516 entries, 0 to 788515\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   ID      788516 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 6.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated articles\n",
    "IDs_nodup=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/IDs_no_duplicates.csv')\n",
    "print(IDs_nodup.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 788516 entries, 0 to 788515\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   ID                           788516 non-null  int64 \n",
      " 1   RegSentsExpand               788516 non-null  object\n",
      " 2   NounChunksMatch              788516 non-null  int64 \n",
      " 3   NounChunkMatchWords          788516 non-null  object\n",
      " 4   NounChunkMatchWordsFiltered  788516 non-null  object\n",
      " 5   NounChunkMatchFiltered       788516 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 36.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_regSentsExpand['ID']=df_regSentsExpand['ID'].astype('int64')\n",
    "df_regSentsExpand=IDs_nodup.merge(df_regSentsExpand,on='ID',how='left').reset_index(drop=True)\n",
    "print(df_regSentsExpand.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 493418 entries, 0 to 493417\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   ID                           493418 non-null  int64 \n",
      " 1   RegSentsExpand               493418 non-null  object\n",
      " 2   NounChunksMatch              493418 non-null  int64 \n",
      " 3   NounChunkMatchWords          493418 non-null  object\n",
      " 4   NounChunkMatchWordsFiltered  493418 non-null  object\n",
      " 5   NounChunkMatchFiltered       493418 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 22.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Refine to reg relevant articles\n",
    "df_regSentsExpandRelevant=df_regSentsExpand[df_regSentsExpand['NounChunkMatchFiltered']>0].reset_index(drop=True)\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Link Expanded Reg Sentences to Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: dominant distinct area (dda): use the dominant areas from area-specific noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/my_py/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ec2-user/SageMaker/.conda/envs/my_py/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/home/ec2-user/SageMaker/.conda/envs/my_py/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df_regSentsExpandRelevant['AllDistinctAreas']=''\n",
    "df_regSentsExpandRelevant['DistinctAreaCount']=''\n",
    "df_regSentsExpandRelevant['DominantDistinctArea']=''\n",
    "for i in range(0, len(df_regSentsExpandRelevant)):\n",
    "    nounchunks=df_regSentsExpandRelevant['NounChunkMatchWordsFiltered'][i]\n",
    "    area_list=[]\n",
    "    for nc in nounchunks:\n",
    "        if nc in nounchunks_area_dict:\n",
    "            area=sorted(literal_eval(nounchunks_area_dict[nc]))\n",
    "            if len(area)==1:\n",
    "                area_list=area_list+area    \n",
    "    \n",
    "    area_count=Counter(area_list).most_common()\n",
    "    dominant_area=[j for j in Counter(area_list).keys() if area_list.count(j)==max(Counter(area_list).values())]\n",
    "    df_regSentsExpandRelevant['AllDistinctAreas'][i]=area_list\n",
    "    df_regSentsExpandRelevant['DistinctAreaCount'][i]=area_count\n",
    "    df_regSentsExpandRelevant['DominantDistinctArea'][i]=dominant_area\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_regSentsExpandRelevant.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regSentsExpandRelevant.to_pickle('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegSentsExpand_NounChunks_Area_Robust2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtered Noun Chunk Occurences by Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg relevant articles\n",
    "df_regSentsExpandRelevant=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegSentsExpand_NounChunks_Area_Robust2.pkl')\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered noun chunk occurences across regulation-related articles\n",
    "df_nounchunk_occurences=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/RegSentsExpand_FilteredNounChunkOccurences.csv')\n",
    "print(df_nounchunk_occurences.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies by dominant distinct area (dda)\n",
    "for i in range(1,15):\n",
    "    var='DominantDistinctArea'+str(i)\n",
    "    df_regSentsExpandRelevant[var]=0\n",
    "    for j in range(0, len(df_regSentsExpandRelevant)):\n",
    "        if i in df_regSentsExpandRelevant['DominantDistinctArea'][j]:\n",
    "            df_regSentsExpandRelevant[var][j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered noun chunks across regulation-related articles by area\n",
    "for i in range(1,15):\n",
    "    allDistinctAreas=[]\n",
    "    for allArea in df_regSentsExpandRelevant[df_regSentsExpandRelevant['DominantDistinctArea'+str(i)]==1]['NounChunkMatchWordsFiltered']:\n",
    "        distinctArea=[nc for nc in allArea if nc in nounchunks_area_dict]\n",
    "        allDistinctAreas=allDistinctAreas+distinctArea\n",
    "    allDistinctAreaCount=Counter(allDistinctAreas)\n",
    "    var_name='Occurences_dda'+str(i)\n",
    "    df_MatchWords = pd.DataFrame(allDistinctAreaCount.items(),columns = ['Noun Chunks',var_name])\n",
    "    df_nounchunk_occurences=df_nounchunk_occurences.merge(df_MatchWords,on='Noun Chunks',how='outer')\n",
    "print(df_nounchunk_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nounchunk_occurences.to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_FilteredNounChunkOccurences_Robust2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg relevant articles\n",
    "df_regSentsExpandRelevant=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegSentsExpand_NounChunks_Area_Robust2.pkl')\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All news data\n",
    "allNews=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/parsed_xml.pkl')\n",
    "print(allNews.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with all news data\n",
    "allNews['ID']=allNews['ID'].astype('int64')\n",
    "df=df_regSentsExpandRelevant.merge(allNews[['ID','Title','Type','SourceType','StartDate','Newspaper','Year','Month']],\n",
    "                                  on='ID',how='left')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns for different approaches\n",
    "area_range=15    # Number of areas + 1\n",
    "col_list=[]\n",
    "for i in range(1,area_range):\n",
    "    var='DominantDistinctArea'+str(i)\n",
    "    col_list.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies by dominant distinct area (dda)\n",
    "for i in range(1,area_range):\n",
    "    var='DominantDistinctArea'+str(i)\n",
    "    df[var]=0\n",
    "    for j in range(0, len(df)):\n",
    "        if i in df['DominantDistinctArea'][j]:\n",
    "            df[var][j]=1\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with sentiment scores\n",
    "sentimentScores=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Sentiment Analysis/RegRelevant_ArticleSentimentScores.csv')\n",
    "print(sentimentScores.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "df['ID']=df['ID'].astype('int64')\n",
    "df2=df.merge(sentimentScores[['ID','UncertaintyScore','GIscore','LMscore','LSDscore']],on='ID',how='left')\n",
    "#print(df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['ID','StartDate','Newspaper','UncertaintyScore','GIscore','LMscore','LSDscore']+col_list].\\\n",
    "    to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_ArticleSentimentScores_Robust2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monthly article count by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate monthly article counts\n",
    "monthlyAreaCount=df[['Newspaper','Year','Month']+col_list].groupby(['Newspaper','Year','Month']).agg('sum').reset_index()\n",
    "print(monthlyAreaCount.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyAreaCount.to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_MonthlyArticleCountByNewspaper_Robust2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct categorical sentiment indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_ArticleSentimentScores_Robust2.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data\n",
    "df['StartDate']=df['StartDate'].astype('datetime64[ns]')\n",
    "df['Year']=df['StartDate'].dt.year\n",
    "df['Month']=df['StartDate'].dt.month\n",
    "df['Newspaper']=df['Newspaper'].astype('category')\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year-month dataframe\n",
    "df_ym=df[['Year','Month']].drop_duplicates().sort_values(['Year','Month']).reset_index(drop=True).reset_index()\n",
    "df_ym['YM']=df_ym['index']+1\n",
    "df_ym['YM']=df_ym['YM'].astype('str')\n",
    "df_ym=df_ym.drop('index',axis=1)\n",
    "print(df_ym,'\\n',len(df_ym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge year-month dataframe\n",
    "df=df.merge(df_ym[['Year','Month','YM']],on=['Year','Month'],how='left').sort_values(['Year','Month']).reset_index(drop=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'UncertaintyScore':'Uncertaintyscore'})\n",
    "YM_list=df_ym['YM'].tolist()\n",
    "#print(YM_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function (suppressing constant) to estimate categorical index\n",
    "def estimate_categorical_index(score, area):\n",
    "    df_area=df[df[area]==1].reset_index(drop=True)\n",
    "    FE_OLS=sm.ols(formula=score + ' ~ 0+C(YM)+C(Newspaper)', data=df_area).fit()\n",
    "    #print(FE_OLS.summary())\n",
    "\n",
    "    FE_estimates=pd.DataFrame()\n",
    "    new_var=score.split('score')[0]+'_'+area\n",
    "    FE_estimates[new_var]=FE_OLS.params[0:len(df_ym)]\n",
    "    FE_estimates=FE_estimates.reset_index().rename(columns={'index':'FE'})\n",
    "    FE_estimates['YM']=FE_estimates['FE'].str.split(\"[\",expand=True)[1].str.split(\"]\",expand=True)[0]\n",
    "    \n",
    "    for value in FE_estimates['YM']:\n",
    "        if value not in YM_list:\n",
    "            FE_estimates=FE_estimates[FE_estimates['YM']!=value]\n",
    "    FE_estimates=FE_estimates.drop('FE',axis=1)\n",
    "    \n",
    "    return FE_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns for different classification approaches\n",
    "area_range=15\n",
    "area_list=[]\n",
    "for i in range(1,area_range):\n",
    "    var='DominantDistinctArea'+str(i)\n",
    "    area_list.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Uncertainty Index\n",
    "CategoricalUncertaintyIndex=df_ym\n",
    "for area in area_list:\n",
    "    try:\n",
    "        estimates=estimate_categorical_index('Uncertaintyscore', area)\n",
    "        CategoricalUncertaintyIndex=CategoricalUncertaintyIndex.merge(estimates,on='YM',how='left')\n",
    "    except:\n",
    "        print(\"Failed:\",area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoricalUncertaintyIndex.to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_MonthlyUncertaintyIndex_Robust2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical sentiment indexes\n",
    "for dict in ['GI','LM','LSD']:\n",
    "    CategoricalSentimentIndex=df_ym\n",
    "    for area in area_list:\n",
    "        try:\n",
    "            estimates=estimate_categorical_index(dict+'score', area)\n",
    "            CategoricalSentimentIndex=CategoricalSentimentIndex.merge(estimates,on='YM',how='left')\n",
    "        except:\n",
    "            print(\"Failed:\",dict+\":\"+area)      \n",
    "    CategoricalSentimentIndex.to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_Monthly'+dict+'Index_Robust2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py",
   "language": "python",
   "name": "my_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
