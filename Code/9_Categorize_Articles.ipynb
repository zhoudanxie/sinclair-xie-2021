{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/nltk_data',\n",
       " '/home/ec2-user/SageMaker/.conda/envs/my_py/nltk_data',\n",
       " '/home/ec2-user/SageMaker/.conda/envs/my_py/share/nltk_data',\n",
       " '/home/ec2-user/SageMaker/.conda/envs/my_py/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Regulatory Sections and Noun Chunks with Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10645 entries, 0 to 10644\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   nc_code      10645 non-null  int64 \n",
      " 1   noun_chunks  10645 non-null  object\n",
      " 2   rin          10645 non-null  object\n",
      " 3   area         10185 non-null  object\n",
      " 4   area_no      10645 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 415.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Noun chunks with areas\n",
    "nounchunks_area=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/DictionaryOfRegulatoryNounChunks.csv')\n",
    "print(nounchunks_area.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nc_code</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>rin</th>\n",
       "      <th>area</th>\n",
       "      <th>area_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>180-day exclusivity</td>\n",
       "      <td>0910-AC11</td>\n",
       "      <td>{1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1983 amendment</td>\n",
       "      <td>2115-AB72</td>\n",
       "      <td>{2}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1988 amendment</td>\n",
       "      <td>1205-AB05</td>\n",
       "      <td>{4}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1990 farm bill</td>\n",
       "      <td>0584-AB28</td>\n",
       "      <td>{1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1993 provision</td>\n",
       "      <td>0970-AB32,3206-AG31</td>\n",
       "      <td>{1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nc_code          noun_chunks                  rin area  area_no\n",
       "0        0  180-day exclusivity            0910-AC11  {1}        1\n",
       "1        1       1983 amendment            2115-AB72  {2}        1\n",
       "2        2       1988 amendment            1205-AB05  {4}        1\n",
       "3        3       1990 farm bill            0584-AB28  {1}        1\n",
       "4        4       1993 provision  0970-AB32,3206-AG31  {1}        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nounchunks_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10185\n"
     ]
    }
   ],
   "source": [
    "# Convert to dictionary\n",
    "nounchunks_area=nounchunks_area[nounchunks_area['area_no']>0].set_index('noun_chunks')\n",
    "nounchunks_area_dict=nounchunks_area.to_dict()['area']\n",
    "print(len(nounchunks_area_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 822737 entries, 0 to 822736\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   ID                           822737 non-null  object\n",
      " 1   RegSentsExpand               822737 non-null  object\n",
      " 2   NounChunksMatch              822737 non-null  int64 \n",
      " 3   NounChunkMatchWords          822737 non-null  object\n",
      " 4   NounChunkMatchWordsFiltered  822737 non-null  object\n",
      " 5   NounChunkMatchFiltered       822737 non-null  int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 37.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Expanded reg sentences with matched noun chunks\n",
    "df_regSentsExpand=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/RegSentsExpand_NounChunks3.pkl')\n",
    "print(df_regSentsExpand.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 788516 entries, 0 to 788515\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   ID      788516 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 6.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated articles\n",
    "IDs_nodup=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/IDs_no_duplicates.csv')\n",
    "print(IDs_nodup.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 788516 entries, 0 to 788515\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   ID                           788516 non-null  int64 \n",
      " 1   RegSentsExpand               788516 non-null  object\n",
      " 2   NounChunksMatch              788516 non-null  int64 \n",
      " 3   NounChunkMatchWords          788516 non-null  object\n",
      " 4   NounChunkMatchWordsFiltered  788516 non-null  object\n",
      " 5   NounChunkMatchFiltered       788516 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 36.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_regSentsExpand['ID']=df_regSentsExpand['ID'].astype('int64')\n",
    "df_regSentsExpand=IDs_nodup.merge(df_regSentsExpand,on='ID',how='left').reset_index(drop=True)\n",
    "print(df_regSentsExpand.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 493418 entries, 0 to 493417\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   ID                           493418 non-null  int64 \n",
      " 1   RegSentsExpand               493418 non-null  object\n",
      " 2   NounChunksMatch              493418 non-null  int64 \n",
      " 3   NounChunkMatchWords          493418 non-null  object\n",
      " 4   NounChunkMatchWordsFiltered  493418 non-null  object\n",
      " 5   NounChunkMatchFiltered       493418 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 22.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Refine to reg relevant articles\n",
    "df_regSentsExpandRelevant=df_regSentsExpand[df_regSentsExpand['NounChunkMatchFiltered']>0].reset_index(drop=True)\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Link Expanded Reg Sentences to Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approch 1 - dominant noun chunk area (dnca): use all unique areas from dominant noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/my_py/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ec2-user/SageMaker/.conda/envs/my_py/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "df_regSentsExpandRelevant['DominantNounChunk']=''\n",
    "df_regSentsExpandRelevant['DominantNounChunkArea']=''\n",
    "for i in range(0, len(df_regSentsExpandRelevant)):\n",
    "    nc_list=df_regSentsExpandRelevant['NounChunkMatchWordsFiltered'][i]\n",
    "    dominant_nc=[j for j in Counter(nc_list).keys() if nc_list.count(j)==max(Counter(nc_list).values())]\n",
    "    dominant_nc_area=set()\n",
    "    for nc in dominant_nc:\n",
    "        if nc in nounchunks_area_dict:\n",
    "            area=literal_eval(nounchunks_area_dict[nc])\n",
    "            dominant_nc_area.update(area)\n",
    "    df_regSentsExpandRelevant['DominantNounChunk'][i]=dominant_nc\n",
    "    df_regSentsExpandRelevant['DominantNounChunkArea'][i]=dominant_nc_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - dominant area (da): use the dominant areas from all noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regSentsExpandRelevant['AllAreas']=''\n",
    "for i in range(0, len(df_regSentsExpandRelevant)):\n",
    "    nounchunks=df_regSentsExpandRelevant['NounChunkMatchWordsFiltered'][i]\n",
    "    area_list=[]\n",
    "    for nc in nounchunks:\n",
    "        if nc in nounchunks_area_dict:\n",
    "            area=sorted(literal_eval(nounchunks_area_dict[nc]))\n",
    "            area_list=area_list+area\n",
    "    df_regSentsExpandRelevant['AllAreas'][i]=area_list\n",
    "\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regSentsExpandRelevant['AreaCount']=''\n",
    "df_regSentsExpandRelevant['DominantArea']=''\n",
    "for i in range(0, len(df_regSentsExpandRelevant)):\n",
    "    area_list=df_regSentsExpandRelevant['AllAreas'][i]\n",
    "    area_count=Counter(area_list).most_common()\n",
    "    dominant_area=[j for j in Counter(area_list).keys() if area_list.count(j)==max(Counter(area_list).values())]\n",
    "    df_regSentsExpandRelevant['AreaCount'][i]=area_count\n",
    "    df_regSentsExpandRelevant['DominantArea'][i]=dominant_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3 - unique distinct area (uda): use all unique areas from area-specific noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regSentsExpandRelevant['AllDistinctAreas']=''\n",
    "df_regSentsExpandRelevant['UniqueDistinctAreas']=''\n",
    "for i in range(0, len(df_regSentsExpandRelevant)):\n",
    "    nounchunks=df_regSentsExpandRelevant['NounChunkMatchWordsFiltered'][i]\n",
    "    area_list=[]\n",
    "    area_set=set()\n",
    "    for nc in nounchunks:\n",
    "        if nc in nounchunks_area_dict:\n",
    "            area=sorted(literal_eval(nounchunks_area_dict[nc]))\n",
    "            if len(area)==1:\n",
    "                area_list=area_list+area\n",
    "    area_set=set(area_list)\n",
    "    df_regSentsExpandRelevant['AllDistinctAreas'][i]=area_list\n",
    "    df_regSentsExpandRelevant['UniqueDistinctAreas'][i]=area_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: dominant distinct area (dda): use the dominant areas from area-specific noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regSentsExpandRelevant['DistinctAreaCount']=''\n",
    "df_regSentsExpandRelevant['DominantDistinctArea']=''\n",
    "for i in range(0, len(df_regSentsExpandRelevant)):\n",
    "    area_list=df_regSentsExpandRelevant['AllDistinctAreas'][i]\n",
    "    area_count=Counter(area_list).most_common()\n",
    "    dominant_area=[j for j in Counter(area_list).keys() if area_list.count(j)==max(Counter(area_list).values())]\n",
    "    df_regSentsExpandRelevant['DistinctAreaCount'][i]=area_count\n",
    "    df_regSentsExpandRelevant['DominantDistinctArea'][i]=dominant_area\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_regSentsExpandRelevant.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regSentsExpandRelevant.to_pickle('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegSentsExpand_NounChunks_Area.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtered Noun Chunk Occurences by Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg relevant articles\n",
    "df_regSentsExpandRelevant=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegSentsExpand_NounChunks_Area.pkl')\n",
    "print(df_regSentsExpandRelevant.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered noun chunk occurences across regulation-related articles\n",
    "df_nounchunk_occurences=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Reg Relevance/RegSentsExpand_FilteredNounChunkOccurences.csv')\n",
    "print(df_nounchunk_occurences.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies by dominant distinct area (dda)\n",
    "for i in range(1,15):\n",
    "    var='DominantDistinctArea'+str(i)\n",
    "    df_regSentsExpandRelevant[var]=0\n",
    "    for j in range(0, len(df_regSentsExpandRelevant)):\n",
    "        if i in df_regSentsExpandRelevant['DominantDistinctArea'][j]:\n",
    "            df_regSentsExpandRelevant[var][j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered noun chunks across regulation-related articles by area\n",
    "for i in range(1,15):\n",
    "    allMatchWords=[]\n",
    "    for list in df_regSentsExpandRelevant[df_regSentsExpandRelevant['DominantDistinctArea'+str(i)]==1]['NounChunkMatchWordsFiltered']:\n",
    "        allMatchWords=allMatchWords+list\n",
    "    allMatchWordsCount=Counter(allMatchWords)\n",
    "    var_name='Occurences_dda'+str(i)\n",
    "    df_MatchWords = pd.DataFrame(allMatchWordsCount.items(),columns = ['Noun Chunks',var_name])\n",
    "    df_nounchunk_occurences=df_nounchunk_occurences.merge(df_MatchWords,on='Noun Chunks',how='outer')\n",
    "print(df_nounchunk_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nounchunk_occurences.to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_FilteredNounChunkOccurences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nounchunk_occurences=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_FilteredNounChunkOccurences.csv')\n",
    "# print(df_nounchunk_occurences.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine Areas 8 & 9\n",
    "# df_regSentsExpandRelevant['DominantDistinctArea8_9']=0\n",
    "# df_regSentsExpandRelevant.loc[(df_regSentsExpandRelevant['DominantDistinctArea8']==1) | (df_regSentsExpandRelevant['DominantDistinctArea9']==1),\n",
    "#                              'DominantDistinctArea8_9']=1\n",
    "# print(df_regSentsExpandRelevant[['DominantDistinctArea8_9']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filtered noun chunks across regulation-related articles for area8_9\n",
    "# allMatchWords=[]\n",
    "# for list in df_regSentsExpandRelevant[df_regSentsExpandRelevant['DominantDistinctArea8_9']==1]['NounChunkMatchWordsFiltered']:\n",
    "#     allMatchWords=allMatchWords+list\n",
    "# allMatchWordsCount=Counter(allMatchWords)\n",
    "# var_name='Occurences_dda8_9'\n",
    "# df_MatchWords = pd.DataFrame(allMatchWordsCount.items(),columns = ['Noun Chunks',var_name])\n",
    "# df_nounchunk_occurences=df_nounchunk_occurences.merge(df_MatchWords,on='Noun Chunks',how='outer')\n",
    "# print(df_nounchunk_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nounchunk_occurences.to_csv('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegArea_FilteredNounChunkOccurences.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Examine the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the results\n",
    "df=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Categorical Index/RegSentsExpand_NounChunks_Area.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50000,55000):\n",
    "    if len(df['AllDistinctAreas'][i])>1 and len(df['DominantDistinctArea'][i])==1:\n",
    "        print(df['ID'][i], df['RegSentsExpand'][i],df['NounChunkMatchWordsFiltered'][i],df['AllDistinctAreas'][i], df['DominantDistinctArea'][i])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['ID']=='294286922']['NounChunkMatchWordsFiltered'].values)\n",
    "print(df[df['ID']=='294286922'][['AllAreas','DominantArea']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200000,205000):\n",
    "    if len(df['AllDistinctAreas'][i])>1 and len(df['DominantDistinctArea'][i])==1 and (len(df['RegSentsExpand'][i])<600):\n",
    "        print(df['ID'][i], df['RegSentsExpand'][i],df['NounChunkMatchWordsFiltered'][i],df['AllDistinctAreas'][i], df['DominantDistinctArea'][i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['ID']=='282489466']['NounChunkMatchWordsFiltered'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py",
   "language": "python",
   "name": "my_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
